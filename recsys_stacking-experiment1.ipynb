{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# import geoplot as gplt\n",
    "from shapely.geometry import Point\n",
    "import shapely\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13.7,10.27)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_color_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask\n",
    "\n",
    "from ast import literal_eval\n",
    "from collections.abc import MutableMapping\n",
    "from collections import Counter, defaultdict\n",
    "# import h5py\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import BaselineOnly\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection.split import train_test_split\n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, io\n",
    "from surprise import KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\n",
    "from surprise import SVDpp, SVD, NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise import accuracy\n",
    "from sklearn.linear_model import Ridge, RidgeCV, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_ALGO = \"SVD\"\n",
    "BLENDER_RIDGE = \"RidgeCV\"\n",
    "BLENDER_SGD = \"SGDRegressor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:44781</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>33.65 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:44781' processes=8 threads=16, memory=33.65 GB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(n_workers=8)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rating', 'reviewerName', 'categories', 'gPlusPlaceId', 'gPlusUserId',\n",
       "       'user_lat', 'user_long', 'placeName', 'price', 'address', 'place_lat',\n",
       "       'place_long'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df = dd.read_csv('data/joined_df.csv').compute()\n",
    "joined_df = joined_df.loc[:, ~joined_df.columns.str.match('Unnamed')]\n",
    "joined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def places_filter(df, lat_l, lat_h, lon_l, lon_h):\n",
    "    return df[(lat_l <= df.place_lat ) & (df.place_lat <= lat_h) & (lon_l <= df.place_long) & (df.place_long <= lon_h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 208 ms, sys: 40 ms, total: 248 ms\n",
      "Wall time: 239 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2285757, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "usa_df = places_filter(joined_df, 19.50139, 64.85694, -161.75583, -68.01197)\n",
    "usa_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set\n",
    "reader = Reader(rating_scale=(1,5))  #invoke reader instance of surprise library\n",
    "data = Dataset.load_from_df(usa_df[['gPlusUserId','gPlusPlaceId','rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Stacking for Collaborative filtering RecSys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training first layer for Stacking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_first_layer(algorithms, hparams, train_set, test_set):\n",
    "    trained_algorithms = []\n",
    "    performances = []\n",
    "    \n",
    "    for idx, algorithm in enumerate(algorithms):\n",
    "        hparam = hparams[idx]\n",
    "        print(\"\\t\\t{} training started: {}\".format(idx+1, hparam))\n",
    "        if algorithm is SVD or algorithms is SVDpp: configured_algorithm = algorithm(n_factors=hparam[\"n_factors\"], n_epochs=hparam[\"n_epochs\"], reg_all= hparam[\"reg_all\"])\n",
    "        elif algorithm is NMF: configured_algorithm = algorithm(n_factors=hparam[\"n_factors\"], n_epochs=hparam[\"n_epochs\"])\n",
    "        elif algorithm is SlopeOne: configured_algorithm = algorithm()\n",
    "        elif algorithm is CoClustering: configured_algorithm = algorithm(n_cltr_u=hparam[\"n_cltr_u\"], n_cltr_i=hparam[\"n_cltr_i\"], n_epochs=hparam[\"n_epochs\"], random_state=42)\n",
    "        configured_algorithm.fit(train_set)\n",
    "        trained_algorithms.append(configured_algorithm)\n",
    "        rmse = accuracy.rmse(configured_algorithm.test(test_set), verbose=False)\n",
    "        print(\"\\t\\tRMSE= \", rmse)\n",
    "        performances.append(rmse)\n",
    "    \n",
    "    return trained_algorithms, performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_first_layer(trained_algorithms, test_set):\n",
    "    return [\n",
    "        trained_algorithm.test(test_set)\n",
    "        for trained_algorithm in trained_algorithms\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_last_layer(trained_algorithms, train_set, blender_algorithm=BLENDER_RIDGE):\n",
    "    actual_trainingSet = [each for each in train_set.all_ratings()]\n",
    "    predictions_for_trainingSet = predict_first_layer(trained_algorithms, actual_trainingSet)\n",
    "    print(\"\\t\\tGenerating predictions Complete !\")\n",
    "    \n",
    "    train_pred = [[each.est for each in prediction] for prediction in predictions_for_trainingSet]\n",
    "    train_true = [each.r_ui for each in predictions_for_trainingSet[0]]\n",
    "    \n",
    "    blender_train_X = np.column_stack(train_pred)\n",
    "    blender_train_y = train_true\n",
    "    print(\"\\t\\tOrganizing data for Blender Complete !\")\n",
    "    \n",
    "    assert blender_train_X.shape[0] == len(blender_train_y), \"There's problem in dimension for training set\"\n",
    "    \n",
    "    blender = None\n",
    "    if blender_algorithm is BLENDER_RIDGE: blender = RidgeCV(cv=5)\n",
    "    elif blender_algorithm is BLENDER_SGD: blender = SGDRegressor(max_iter=5000)\n",
    "    print(\"\\t\\tDetermination for Blender Algorithm Complete !\")    \n",
    "    \n",
    "    blender.fit(blender_train_X, blender_train_y)\n",
    "    print(\"\\t\\tBlender Training Complete !\")\n",
    "    return blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_last_layer(trained_algorithms, blender, userID, iid):\n",
    "    preds = [algo.predict(userID, iid) for algo in trained_algorithms]\n",
    "    blender_X = np.column_stack((pred.est for pred in preds))\n",
    "    return blender.predict(blender_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV(cv, algorithms, hparams):\n",
    "    individual_performances = []\n",
    "    sgd_performances = []\n",
    "    ridge_performances = []\n",
    "    \n",
    "    for idx in range(cv):\n",
    "        print(\"{} Cross Validation started\".format(idx+1))\n",
    "        trainingSet, testSet = train_test_split(data, test_size=0.2, train_size=None, random_state=42, shuffle=True)\n",
    "        print(\"\\tData Perparation Completed\")\n",
    "        \n",
    "        trained_algorithms, performances = train_first_layer(algorithms, hparams, trainingSet, testSet)\n",
    "        individual_performances.append(performances)\n",
    "        \n",
    "        predictions = predict_first_layer(trained_algorithms, testSet)\n",
    "        test_true = [each.r_ui for each in predictions[0]]\n",
    "        test_pred = [[each.est for each in prediction] for prediction in predictions]\n",
    "        blender_test_X = np.column_stack(test_pred)\n",
    "        blender_test_y = test_true\n",
    "        print(\"\\tPreparing data for Blender Completed\")\n",
    "        \n",
    "        SGD_blender = train_last_layer(trained_algorithms, trainingSet, BLENDER_SGD)\n",
    "        final_pred = SGD_blender.predict(blender_test_X)\n",
    "        sgd_performance = math.sqrt(mse(final_pred, blender_test_y))\n",
    "        sgd_performances.append(sgd_performance)\n",
    "        print(\"\\tSGD Performance: \", sgd_performance)\n",
    "        \n",
    "        RIDGE_blender = train_last_layer(trained_algorithms, trainingSet, BLENDER_RIDGE)\n",
    "        final_pred = RIDGE_blender.predict(blender_test_X)\n",
    "        ridge_performance = math.sqrt(mse(final_pred, blender_test_y))\n",
    "        ridge_performances.append(ridge_performance)\n",
    "        print(\"\\tRidge Performance: \", ridge_performance)\n",
    "    \n",
    "    return individual_performances, sgd_performances, ridge_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Cross Validation started\n",
      "\tData Perparation Completed\n",
      "\t\t1 training started: {'n_factors': 20, 'n_epochs': 20, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.104058928221667\n",
      "\t\t2 training started: {'n_factors': 10, 'n_epochs': 10, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1163021178376735\n",
      "\t\t3 training started: {'n_factors': 40, 'n_epochs': 40, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1024902835441712\n",
      "\tPreparing data for Blender Completed\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tSGD Performance:  1.108210460939158\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tRidge Performance:  1.1741878518117768\n",
      "2 Cross Validation started\n",
      "\tData Perparation Completed\n",
      "\t\t1 training started: {'n_factors': 20, 'n_epochs': 20, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1041334500931705\n",
      "\t\t2 training started: {'n_factors': 10, 'n_epochs': 10, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1163754128751944\n",
      "\t\t3 training started: {'n_factors': 40, 'n_epochs': 40, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1025266403642986\n",
      "\tPreparing data for Blender Completed\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tSGD Performance:  1.105288739170464\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tRidge Performance:  1.1741878518117768\n",
      "3 Cross Validation started\n",
      "\tData Perparation Completed\n",
      "\t\t1 training started: {'n_factors': 20, 'n_epochs': 20, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1040176547291456\n",
      "\t\t2 training started: {'n_factors': 10, 'n_epochs': 10, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1163519102615207\n",
      "\t\t3 training started: {'n_factors': 40, 'n_epochs': 40, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1023872126523622\n",
      "\tPreparing data for Blender Completed\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tSGD Performance:  1.1033749008833493\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tRidge Performance:  1.1741878518117768\n"
     ]
    }
   ],
   "source": [
    "# Configuration 1\n",
    "algorithms = [SVD, SVD, SVD]\n",
    "hparams = [\n",
    "    {\n",
    "        \"n_factors\": 20,\n",
    "        \"n_epochs\": 20,\n",
    "        \"reg_all\": 0.02\n",
    "    },\n",
    "    {\n",
    "        \"n_factors\": 10,\n",
    "        \"n_epochs\": 10,\n",
    "        \"reg_all\": 0.02\n",
    "    },\n",
    "    {\n",
    "        \"n_factors\": 40,\n",
    "        \"n_epochs\": 40,\n",
    "        \"reg_all\": 0.02\n",
    "    }\n",
    "]\n",
    "individual_performances1, sgd_performances1, ridge_performances1 = CV(3, algorithms, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Cross Validation started\n",
      "\tData Perparation Completed\n",
      "\t\t1 training started: {'n_factors': 20, 'n_epochs': 20, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.104169452163266\n",
      "\t\t2 training started: {'n_factors': 10, 'n_epochs': 10, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.104018660021849\n",
      "\t\t3 training started: {'n_factors': 10, 'n_epochs': 10}\n",
      "\t\tRMSE=  1.5191963001814524\n",
      "\tPreparing data for Blender Completed\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tSGD Performance:  1.163313891590333\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tRidge Performance:  1.1741878518117768\n",
      "2 Cross Validation started\n",
      "\tData Perparation Completed\n",
      "\t\t1 training started: {'n_factors': 20, 'n_epochs': 20, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1041579396997705\n",
      "\t\t2 training started: {'n_factors': 10, 'n_epochs': 10, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1040242837478833\n",
      "\t\t3 training started: {'n_factors': 10, 'n_epochs': 10}\n",
      "\t\tRMSE=  1.5186261119582278\n",
      "\tPreparing data for Blender Completed\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tSGD Performance:  1.1551716076213816\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tRidge Performance:  1.1741878518117768\n",
      "3 Cross Validation started\n",
      "\tData Perparation Completed\n",
      "\t\t1 training started: {'n_factors': 20, 'n_epochs': 20, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1040802393895066\n",
      "\t\t2 training started: {'n_factors': 10, 'n_epochs': 10, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1040675942725207\n",
      "\t\t3 training started: {'n_factors': 10, 'n_epochs': 10}\n",
      "\t\tRMSE=  1.520093995363411\n",
      "\tPreparing data for Blender Completed\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tSGD Performance:  1.1491646896736907\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tRidge Performance:  1.1741878518117768\n"
     ]
    }
   ],
   "source": [
    "# Configuration 2\n",
    "algorithms = [SVD, SVDpp, NMF]\n",
    "hparams = [\n",
    "    {\n",
    "        \"n_factors\": 20,\n",
    "        \"n_epochs\": 20,\n",
    "        \"reg_all\": 0.02\n",
    "    },\n",
    "    {\n",
    "        \"n_factors\": 10,\n",
    "        \"n_epochs\": 10,\n",
    "        \"reg_all\": 0.02\n",
    "    },\n",
    "    {\n",
    "        \"n_factors\": 10,\n",
    "        \"n_epochs\": 10\n",
    "    }\n",
    "]\n",
    "individual_performances2, sgd_performances2, ridge_performances2 = CV(3, algorithms, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Cross Validation started\n",
      "\tData Perparation Completed\n",
      "\t\t1 training started: {'n_factors': 20, 'n_epochs': 20, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1040136718387685\n",
      "\t\t2 training started: {'n_factors': 10, 'n_epochs': 10}\n",
      "\t\tRMSE=  1.5182970493570633\n",
      "\t\t3 training started: {'n_cltr_u': 4, 'n_cltr_i': 4, 'n_epochs': 30}\n",
      "\t\tRMSE=  1.2169442432729836\n",
      "\tPreparing data for Blender Completed\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tSGD Performance:  1.1919274541577873\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tRidge Performance:  1.1741878518117768\n",
      "2 Cross Validation started\n",
      "\tData Perparation Completed\n",
      "\t\t1 training started: {'n_factors': 20, 'n_epochs': 20, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.1040702709711407\n",
      "\t\t2 training started: {'n_factors': 10, 'n_epochs': 10}\n",
      "\t\tRMSE=  1.5179731608544629\n",
      "\t\t3 training started: {'n_cltr_u': 4, 'n_cltr_i': 4, 'n_epochs': 30}\n",
      "\t\tRMSE=  1.2169442432729836\n",
      "\tPreparing data for Blender Completed\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tSGD Performance:  1.172098305204927\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tRidge Performance:  1.1741878518117768\n",
      "3 Cross Validation started\n",
      "\tData Perparation Completed\n",
      "\t\t1 training started: {'n_factors': 20, 'n_epochs': 20, 'reg_all': 0.02}\n",
      "\t\tRMSE=  1.104108333964276\n",
      "\t\t2 training started: {'n_factors': 10, 'n_epochs': 10}\n",
      "\t\tRMSE=  1.5206353514558368\n",
      "\t\t3 training started: {'n_cltr_u': 4, 'n_cltr_i': 4, 'n_epochs': 30}\n",
      "\t\tRMSE=  1.2169442432729836\n",
      "\tPreparing data for Blender Completed\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tSGD Performance:  1.1653601229616366\n",
      "\t\tGenerating predictions Complete !\n",
      "\t\tOrganizing data for Blender Complete !\n",
      "\t\tDetermination for Blender Algorithm Complete !\n",
      "\t\tBlender Training Complete !\n",
      "\tRidge Performance:  1.1741878518117768\n"
     ]
    }
   ],
   "source": [
    "# Configuration 3\n",
    "algorithms = [SVD, NMF, CoClustering]\n",
    "hparams = [\n",
    "    {\n",
    "        \"n_factors\": 20,\n",
    "        \"n_epochs\": 20,\n",
    "        \"reg_all\": 0.02\n",
    "    },\n",
    "    {\n",
    "        \"n_factors\": 10,\n",
    "        \"n_epochs\": 10\n",
    "    },\n",
    "    {\n",
    "        \"n_cltr_u\": 4,\n",
    "        \"n_cltr_i\": 4,\n",
    "        \"n_epochs\": 30,\n",
    "    }\n",
    "]\n",
    "individual_performances3, sgd_performances3, ridge_performances3 = CV(3, algorithms, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.104058928221667, 1.1163021178376735, 1.1024902835441712],\n",
       " [1.1041334500931705, 1.1163754128751944, 1.1025266403642986],\n",
       " [1.1040176547291456, 1.1163519102615207, 1.1023872126523622]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_performances1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.108210460939158, 1.105288739170464, 1.1033749008833493]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_performances1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1056247003309905, 0.0019883515264945873)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First Experiment Result for SGD Blender\n",
    "# algorithms = [SVD, SVD, SVD]\n",
    "np.mean(sgd_performances1), np.std(sgd_performances1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.104169452163266, 1.104018660021849, 1.5191963001814524],\n",
       " [1.1041579396997705, 1.1040242837478833, 1.5186261119582278],\n",
       " [1.1040802393895066, 1.1040675942725207, 1.520093995363411]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_performances2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.163313891590333, 1.1551716076213816, 1.1491646896736907]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_performances2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.155883396295135, 0.005798273366933818)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second Experminet Result for SGD Blender\n",
    "# algorithms = [SVD, SVDpp, NMF]\n",
    "np.mean(sgd_performances2), np.std(sgd_performances2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.1040136718387685, 1.5182970493570633, 1.2169442432729836],\n",
       " [1.1040702709711407, 1.5179731608544629, 1.2169442432729836],\n",
       " [1.104108333964276, 1.5206353514558368, 1.2169442432729836]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "individual_performances3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1919274541577873, 1.172098305204927, 1.1653601229616366]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_performances3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1764619607747837, 0.01127643233092059)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third Experiment result for SGD Blender\n",
    "# algorithms = [SVD, NMF, CoClustering]\n",
    "np.mean(sgd_performances3), np.std(sgd_performances3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmpe256-team4",
   "language": "python",
   "name": "cmpe256-team4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
